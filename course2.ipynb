{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "482a38fa-9e56-4d8a-9076-f49622a4c52c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!pip install transformers\n",
    "!pip install datasets\n",
    "\n",
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"yelp_review_full\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0ba64ad-ba97-47c4-b403-068791ab7bd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cd0ad5c-3eb3-4e15-a251-73bc4cbb1af9",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset[\"train\"][10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c219d334-6016-4e48-8054-5d47cc7f340a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import pandas as pd\n",
    "import datasets\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "def show_random_elements(dataset, num_examples=10):\n",
    "    assert num_examples <= len(dataset), \"Can't pick more elements than there are in the dataset.\"\n",
    "    picks = []\n",
    "    for _ in range(num_examples):\n",
    "        pick = random.randint(0, len(dataset)-1)\n",
    "        while pick in picks:\n",
    "            pick = random.randint(0, len(dataset)-1)\n",
    "        picks.append(pick)\n",
    "    \n",
    "    df = pd.DataFrame(dataset[picks])\n",
    "    for column, typ in dataset.features.items():\n",
    "        if isinstance(typ, datasets.ClassLabel):\n",
    "            df[column] = df[column].transform(lambda i: typ.names[i])\n",
    "    display(HTML(df.to_html()))\n",
    "\n",
    "\n",
    "show_random_elements(dataset[\"train\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c378605-72af-4e4e-b557-de655b88d354",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")\n",
    "\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True)\n",
    "\n",
    "\n",
    "tokenized_datasets = dataset.map(tokenize_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "897b3775-029c-4b66-9cda-7d4267670d3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_random_elements(tokenized_datasets[\"train\"], num_examples=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6263eb7d-0474-4544-bcf2-ed7cb9152663",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSequenceClassification\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"bert-base-cased\", num_labels=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1000b644-eb32-47df-9849-23e8d953e41d",
   "metadata": {
    "editable": true,
    "scrolled": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments\n",
    "\n",
    "model_dir = \"models/bert-base-cased\"\n",
    "\n",
    "# logging_steps 默认值为500，根据我们的训练数据和步长，将其设置为100\n",
    "training_args = TrainingArguments(output_dir=f\"{model_dir}/test_trainer\",\n",
    "                                  logging_dir=f\"{model_dir}/test_trainer/runs\",\n",
    "                                  logging_steps=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6aa36c51-2c77-4827-a3dc-4ee4874cc2a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 完整的超参数配置\n",
    "print(training_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "783d9bc2-b298-4be4-94e1-047efcc0c69b",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!pip install evaluate\n",
    "!pip install scikit-learn\n",
    "\n",
    "import numpy as np\n",
    "import evaluate\n",
    "\n",
    "metric = evaluate.load(\"accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70cbc530-1b43-41ea-93a9-7745f6209eb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments, Trainer\n",
    "\n",
    "training_args = TrainingArguments(output_dir=f\"{model_dir}/test_trainer\",\n",
    "                                  evaluation_strategy=\"epoch\", \n",
    "                                  logging_dir=f\"{model_dir}/test_trainer/runs\",\n",
    "                                  logging_steps=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25a20f79-7716-4c16-b041-69a4126079dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip uninstall -y deepspeed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4423210d-aa5c-46d5-944b-b005606659cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://mirrors.ustc.edu.cn/pypi/web/simple\n",
      "Requirement already satisfied: mpi4py in /home/xuwei/jupyterlab/venv/lib/python3.11/site-packages (3.1.5)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.2.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.3.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-01-18 18:42:15,170] [WARNING] [config_utils.py:69:_process_deprecated_field] Config parameter cpu_offload is deprecated use offload_optimizer instead\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='125' max='125' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [125/125 00:55, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.339844</td>\n",
       "      <td>0.427000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# !pip install --upgrade transformers deepspeed\n",
    "# !echo xuwei | sudo apt-get install mpich \n",
    "!pip install mpi4py\n",
    "from transformers import TrainingArguments, Trainer\n",
    "from transformers import AutoModelForSequenceClassification\n",
    "from transformers import AutoTokenizer\n",
    "from datasets import load_dataset\n",
    "import numpy as np\n",
    "import evaluate\n",
    "\n",
    "#model_dir = \"/data/models/bert-base-cased-finetuned\"\n",
    "model_dir = \"models/bert-base-cased\"\n",
    "training_args = TrainingArguments(output_dir=f\"{model_dir}/test_trainer\",\n",
    "                                  evaluation_strategy=\"epoch\", \n",
    "                                  logging_dir=f\"{model_dir}/test_trainer/runs\",\n",
    "                                  fp16=True,\n",
    "                                  save_steps=1000,\n",
    "                                  num_train_epochs=1,\n",
    "                                  deepspeed=\"deepspeed_config.json\",\n",
    "                                  logging_steps=1000)\n",
    "\n",
    "\n",
    "\n",
    "dataset = load_dataset(\"yelp_review_full\")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")\n",
    "\n",
    "metric = evaluate.load(\"accuracy\")\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True)\n",
    "\n",
    "tokenized_datasets = dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    return metric.compute(predictions=predictions, references=labels)\n",
    "\n",
    "#准备训练数据集和测试数据集\n",
    "# train_dataset = tokenized_datasets[\"train\"].shuffle(seed=42).select(range(1000))\n",
    "# eval_dataset = tokenized_datasets[\"test\"].shuffle(seed=42).select(range(1000))\n",
    "train_dataset = tokenized_datasets[\"train\"].shuffle(seed=42)\n",
    "eval_dataset = tokenized_datasets[\"test\"].shuffle(seed=42)\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"bert-base-cased\", num_labels=5)\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "#开始训练\n",
    "trainer.train()\n",
    "trainer.save_model(f\"{model_dir}/finetuned-trainer\")\n",
    "trainer.save_state()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8e5b227-b8fc-4381-96fa-5fce6de99dcf",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!nvidia-smi\n",
    "! sudo kill -9 1778429      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "562e3eeb-4bd7-4223-b85c-e39c8a77cb3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip list | grep torch\n",
    "!echo y | pip uninstall torch\n",
    "!pip install torch==2.0.1\n",
    "!pip list | grep torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dc9e982-6550-4475-8b6d-d786f0b0d8b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import getpass\n",
    "import os\n",
    "\n",
    "password = getpass.getpass()\n",
    "command = \"sudo -S apt-get update\" #can be any command but don't forget -S as it enables input from stdin\n",
    "os.system('echo %s | %s' % (password, command))\n",
    "\n",
    "! sudo apt-get update\n",
    "! sudo apt-get install libopenmpi-dev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c6d2b98-8c9c-4c3f-9e5b-5418e0bf16c9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    " # 为加速的情况[ 781/243750 03:25 < 17:46:44, 3.80 it/s, Epoch 0.01/3]\n",
    "# 尝试使用加速\n",
    "!pip install deepspeed\n",
    "!pip install transformers\n",
    "!pip install datasets\n",
    "!pip install mpi4py\n",
    "\n",
    "from transformers import TrainingArguments, Trainer\n",
    "from transformers import AutoModelForSequenceClassification\n",
    "from transformers import AutoTokenizer\n",
    "from datasets import load_dataset\n",
    "\n",
    "model_dir = \"models/bert-base-cased\"\n",
    "training_args = TrainingArguments(output_dir=f\"{model_dir}/test_trainer\",\n",
    "                                  evaluation_strategy=\"epoch\", \n",
    "                                  logging_dir=f\"{model_dir}/test_trainer/runs\",\n",
    "                                  deepspeed=\"./deepspeed_config.json\",\n",
    "                                  fp16=True, \n",
    "                                  logging_steps=100)\n",
    "\n",
    "\n",
    "\n",
    "dataset = load_dataset(\"yelp_review_full\")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")\n",
    "\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True)\n",
    "\n",
    "tokenized_datasets = dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    return metric.compute(predictions=predictions, references=labels)\n",
    "\n",
    "#准备训练数据集和测试数据集\n",
    "train_dataset = tokenized_datasets[\"train\"].shuffle(seed=42)\n",
    "eval_dataset = tokenized_datasets[\"test\"].shuffle(seed=42)\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"bert-base-cased\", num_labels=5)\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "#开始训练\n",
    "trainer.train()\n",
    "trainer.save_model(f\"{model_dir}/finetuned-trainer\")\n",
    "trainer.save_state()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
