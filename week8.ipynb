{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bcb5c364-5a42-4c89-aadf-77147fc61e0f",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Skipping /home/xuwei/jupyterlab/venv/lib/python3.11/site-packages/charset_normalizer-3.3.2.dist-info due to invalid metadata entry 'name'\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Skipping /home/xuwei/jupyterlab/venv/lib/python3.11/site-packages/requests-2.31.0.dist-info due to invalid metadata entry 'name'\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Skipping /home/xuwei/jupyterlab/venv/lib/python3.11/site-packages/urllib3-2.1.0.dist-info due to invalid metadata entry 'name'\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Skipping /home/xuwei/jupyterlab/venv/lib/python3.11/site-packages/charset_normalizer-3.3.2.dist-info due to invalid metadata entry 'name'\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Skipping /home/xuwei/jupyterlab/venv/lib/python3.11/site-packages/requests-2.31.0.dist-info due to invalid metadata entry 'name'\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Skipping /home/xuwei/jupyterlab/venv/lib/python3.11/site-packages/urllib3-2.1.0.dist-info due to invalid metadata entry 'name'\u001b[0m\u001b[33m\n",
      "\u001b[0mLooking in indexes: https://mirrors.ustc.edu.cn/pypi/web/simple\n",
      "Collecting git+https://github.com/huggingface/transformers\n",
      "  Cloning https://github.com/huggingface/transformers to /tmp/pip-req-build-sbywrgyj\n",
      "  Running command git clone --filter=blob:none --quiet https://github.com/huggingface/transformers /tmp/pip-req-build-sbywrgyj\n",
      "  Resolved https://github.com/huggingface/transformers to commit ce4fff0be7f6464d713f7ac3e0bbaafbc6959ae5\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: filelock in /home/xuwei/jupyterlab/venv/lib/python3.11/site-packages (from transformers==4.38.0.dev0) (3.13.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /home/xuwei/jupyterlab/venv/lib/python3.11/site-packages (from transformers==4.38.0.dev0) (0.20.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/xuwei/jupyterlab/venv/lib/python3.11/site-packages (from transformers==4.38.0.dev0) (1.26.3)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/xuwei/jupyterlab/venv/lib/python3.11/site-packages (from transformers==4.38.0.dev0) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/xuwei/jupyterlab/venv/lib/python3.11/site-packages (from transformers==4.38.0.dev0) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/xuwei/jupyterlab/venv/lib/python3.11/site-packages (from transformers==4.38.0.dev0) (2023.12.25)\n",
      "Requirement already satisfied: requests in /home/xuwei/jupyterlab/venv/lib/python3.11/site-packages (from transformers==4.38.0.dev0) (2.27.1)\n",
      "Requirement already satisfied: tokenizers<0.19,>=0.14 in /home/xuwei/jupyterlab/venv/lib/python3.11/site-packages (from transformers==4.38.0.dev0) (0.15.0)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /home/xuwei/jupyterlab/venv/lib/python3.11/site-packages (from transformers==4.38.0.dev0) (0.4.1)\n",
      "Requirement already satisfied: tqdm>=4.27 in /home/xuwei/jupyterlab/venv/lib/python3.11/site-packages (from transformers==4.38.0.dev0) (4.66.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /home/xuwei/jupyterlab/venv/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.19.3->transformers==4.38.0.dev0) (2023.10.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/xuwei/jupyterlab/venv/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.19.3->transformers==4.38.0.dev0) (4.9.0)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/xuwei/jupyterlab/venv/lib/python3.11/site-packages (from requests->transformers==4.38.0.dev0) (1.26.18)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/xuwei/jupyterlab/venv/lib/python3.11/site-packages (from requests->transformers==4.38.0.dev0) (2023.11.17)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /home/xuwei/jupyterlab/venv/lib/python3.11/site-packages (from requests->transformers==4.38.0.dev0) (2.0.12)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/xuwei/jupyterlab/venv/lib/python3.11/site-packages (from requests->transformers==4.38.0.dev0) (3.6)\n",
      "Building wheels for collected packages: transformers\n",
      "  Building wheel for transformers (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for transformers: filename=transformers-4.38.0.dev0-py3-none-any.whl size=8505457 sha256=09fe5b6a2d7b582bbffffed29594742aa86413ac637279978743d613e37e2c1c\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-gmu_xrzw/wheels/04/a3/f1/b88775f8e1665827525b19ac7590250f1038d947067beba9fb\n",
      "Successfully built transformers\n",
      "\u001b[33mWARNING: Skipping /home/xuwei/jupyterlab/venv/lib/python3.11/site-packages/charset_normalizer-3.3.2.dist-info due to invalid metadata entry 'name'\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Skipping /home/xuwei/jupyterlab/venv/lib/python3.11/site-packages/requests-2.31.0.dist-info due to invalid metadata entry 'name'\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Skipping /home/xuwei/jupyterlab/venv/lib/python3.11/site-packages/urllib3-2.1.0.dist-info due to invalid metadata entry 'name'\u001b[0m\u001b[33m\n",
      "\u001b[0mInstalling collected packages: transformers\n",
      "  Attempting uninstall: transformers\n",
      "\u001b[33m    WARNING: Skipping /home/xuwei/jupyterlab/venv/lib/python3.11/site-packages/charset_normalizer-3.3.2.dist-info due to invalid metadata entry 'name'\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33m    WARNING: Skipping /home/xuwei/jupyterlab/venv/lib/python3.11/site-packages/requests-2.31.0.dist-info due to invalid metadata entry 'name'\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33m    WARNING: Skipping /home/xuwei/jupyterlab/venv/lib/python3.11/site-packages/urllib3-2.1.0.dist-info due to invalid metadata entry 'name'\u001b[0m\u001b[33m\n",
      "\u001b[0m    Found existing installation: transformers 4.36.2\n",
      "    Uninstalling transformers-4.36.2:\n",
      "      Successfully uninstalled transformers-4.36.2\n",
      "\u001b[33mWARNING: Skipping /home/xuwei/jupyterlab/venv/lib/python3.11/site-packages/charset_normalizer-3.3.2.dist-info due to invalid metadata entry 'name'\u001b[0m\u001b[33m\n",
      "\u001b[0mSuccessfully installed transformers-4.38.0.dev0\n",
      "\u001b[33mWARNING: Skipping /home/xuwei/jupyterlab/venv/lib/python3.11/site-packages/charset_normalizer-3.3.2.dist-info due to invalid metadata entry 'name'\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Skipping /home/xuwei/jupyterlab/venv/lib/python3.11/site-packages/transformers-4.36.2.dist-info due to invalid metadata entry 'name'\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Skipping /home/xuwei/jupyterlab/venv/lib/python3.11/site-packages/requests-2.31.0.dist-info due to invalid metadata entry 'name'\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Skipping /home/xuwei/jupyterlab/venv/lib/python3.11/site-packages/urllib3-2.1.0.dist-info due to invalid metadata entry 'name'\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Skipping /home/xuwei/jupyterlab/venv/lib/python3.11/site-packages/charset_normalizer-3.3.2.dist-info due to invalid metadata entry 'name'\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Skipping /home/xuwei/jupyterlab/venv/lib/python3.11/site-packages/transformers-4.36.2.dist-info due to invalid metadata entry 'name'\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Skipping /home/xuwei/jupyterlab/venv/lib/python3.11/site-packages/requests-2.31.0.dist-info due to invalid metadata entry 'name'\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Skipping /home/xuwei/jupyterlab/venv/lib/python3.11/site-packages/urllib3-2.1.0.dist-info due to invalid metadata entry 'name'\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Skipping /home/xuwei/jupyterlab/venv/lib/python3.11/site-packages/charset_normalizer-3.3.2.dist-info due to invalid metadata entry 'name'\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Skipping /home/xuwei/jupyterlab/venv/lib/python3.11/site-packages/transformers-4.36.2.dist-info due to invalid metadata entry 'name'\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Skipping /home/xuwei/jupyterlab/venv/lib/python3.11/site-packages/requests-2.31.0.dist-info due to invalid metadata entry 'name'\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Skipping /home/xuwei/jupyterlab/venv/lib/python3.11/site-packages/urllib3-2.1.0.dist-info due to invalid metadata entry 'name'\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.2.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.0\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# 源代码安装 Transformers\n",
    "# https://huggingface.co/docs/transformers/installation#install-from-source\n",
    "!pip install git+https://github.com/huggingface/transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ef15f349-b00b-416f-81d7-d9a7e38560b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              total        used        free      shared  buff/cache   available\n",
      "Mem:          64307       18103       24509           2       21695       45488\n",
      "Swap:         20479           0       20479\n"
     ]
    }
   ],
   "source": [
    "# 硬件配置如下： v100s 32G显存  64GB内存  交换内存20G（原本交换内存为0，有CPU内存不足导致进程被kill掉，增加交换内存）\n",
    "!free -m\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9357f781-b47c-491b-b934-47d6d711ec63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sat Feb 17 23:04:41 2024       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 525.125.06   Driver Version: 525.125.06   CUDA Version: 12.0     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  Tesla V100-PCIE...  Off  | 00000000:00:0D.0 Off |                    0 |\n",
      "| N/A   31C    P0    24W / 250W |      4MiB / 32768MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|  No running processes found                                                 |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5b7ee97-a02d-4be7-b0d7-25e42a6b6474",
   "metadata": {},
   "outputs": [],
   "source": [
    "参数如下：\n",
    "{\n",
    "    \"fp16\": {\n",
    "        \"enabled\": \"auto\",\n",
    "        \"loss_scale\": 0,\n",
    "        \"loss_scale_window\": 1000,\n",
    "        \"initial_scale_power\": 16,\n",
    "        \"hysteresis\": 2,\n",
    "        \"min_loss_scale\": 1\n",
    "    },\n",
    "\n",
    "    \"bf16\": {\n",
    "        \"enabled\": \"auto\"\n",
    "    },\n",
    "\n",
    "    \"optimizer\": {\n",
    "        \"type\": \"AdamW\",\n",
    "        \"params\": {\n",
    "            \"lr\": \"auto\",\n",
    "            \"betas\": \"auto\",\n",
    "            \"eps\": \"auto\",\n",
    "            \"weight_decay\": \"auto\"\n",
    "        }\n",
    "    },\n",
    "\n",
    "    \"scheduler\": {\n",
    "        \"type\": \"WarmupLR\",\n",
    "        \"params\": {\n",
    "            \"warmup_min_lr\": \"auto\",\n",
    "            \"warmup_max_lr\": \"auto\",\n",
    "            \"warmup_num_steps\": \"auto\"\n",
    "        }\n",
    "    },\n",
    "\n",
    "    \"zero_optimization\": {\n",
    "        \"stage\": 3,\n",
    "        \"offload_optimizer\": {\n",
    "            \"device\": \"cpu\",\n",
    "            \"pin_memory\": false\n",
    "        },\n",
    "\n",
    "        \"overlap_comm\": true,\n",
    "        \"contiguous_gradients\": true,\n",
    "        \"sub_group_size\": 1e8,\n",
    "        \"reduce_bucket_size\": \"auto\",\n",
    "        \"reduce_bucket_size\": 134217720, \n",
    "        \"stage3_prefetch_bucket_size\": 52428800, \n",
    "        \"stage3_max_live_parameters\": 1e9,\n",
    "        \"stage3_max_reuse_distance\": 1e9,\n",
    "        \"stage3_gather_16bit_weights_on_model_save\": true\n",
    "    },\n",
    "\n",
    "    \"gradient_accumulation_steps\": 1, \n",
    "    \"gradient_clipping\": 1.0,\n",
    "    \"steps_per_print\": 20,\n",
    "    \"train_batch_size\": 1, \n",
    "    \"train_micro_batch_size_per_gpu\": 1, \n",
    "    \"wall_clock_breakdown\": false\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5284c454-156a-458b-9957-476857214a6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-02-17 23:17:32,301] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "/home/xuwei/jupyterlab/venv/lib/python3.11/site-packages/requests/__init__.py:102: RequestsDependencyWarning: urllib3 (1.26.18) or chardet (5.2.0)/charset_normalizer (2.0.12) doesn't match a supported version!\n",
      "  warnings.warn(\"urllib3 ({}) or chardet ({})/charset_normalizer ({}) doesn't match a supported \"\n",
      "[2024-02-17 23:17:34,541] [WARNING] [runner.py:202:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.\n",
      "[2024-02-17 23:17:34,541] [INFO] [runner.py:571:main] cmd = /home/xuwei/jupyterlab/venv/bin/python -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbMF19 --master_addr=127.0.0.1 --master_port=29500 --enable_each_rank_log=None translation/run_translation.py --deepspeed config/ds_config_zero3.json --model_name_or_path t5-3b --per_device_train_batch_size 1 --output_dir output_dir --overwrite_output_dir --fp16 --do_train --max_train_samples 500 --num_train_epochs 1 --dataset_name wmt16 --dataset_config ro-en --source_lang en --target_lang ro\n",
      "[2024-02-17 23:17:36,889] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "/home/xuwei/jupyterlab/venv/lib/python3.11/site-packages/requests/__init__.py:102: RequestsDependencyWarning: urllib3 (1.26.18) or chardet (5.2.0)/charset_normalizer (2.0.12) doesn't match a supported version!\n",
      "  warnings.warn(\"urllib3 ({}) or chardet ({})/charset_normalizer ({}) doesn't match a supported \"\n",
      "[2024-02-17 23:17:37,832] [INFO] [launch.py:145:main] WORLD INFO DICT: {'localhost': [0]}\n",
      "[2024-02-17 23:17:37,832] [INFO] [launch.py:151:main] nnodes=1, num_local_procs=1, node_rank=0\n",
      "[2024-02-17 23:17:37,832] [INFO] [launch.py:162:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0]})\n",
      "[2024-02-17 23:17:37,832] [INFO] [launch.py:163:main] dist_world_size=1\n",
      "[2024-02-17 23:17:37,832] [INFO] [launch.py:165:main] Setting CUDA_VISIBLE_DEVICES=0\n",
      "/home/xuwei/jupyterlab/venv/lib/python3.11/site-packages/requests/__init__.py:102: RequestsDependencyWarning: urllib3 (1.26.18) or chardet (5.2.0)/charset_normalizer (2.0.12) doesn't match a supported version!\n",
      "  warnings.warn(\"urllib3 ({}) or chardet ({})/charset_normalizer ({}) doesn't match a supported \"\n",
      "[2024-02-17 23:17:43,836] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "[2024-02-17 23:17:44,048] [INFO] [comm.py:637:init_distributed] cdb=None\n",
      "[2024-02-17 23:17:44,048] [INFO] [comm.py:668:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl\n",
      "02/17/2024 23:17:44 - WARNING - __main__ - Process rank: 0, device: cuda:0, n_gpu: 1, distributed training: True, 16-bits training: True\n",
      "02/17/2024 23:17:44 - INFO - __main__ - Training/evaluation parameters Seq2SeqTrainingArguments(\n",
      "_n_gpu=1,\n",
      "accelerator_config={'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True},\n",
      "adafactor=False,\n",
      "adam_beta1=0.9,\n",
      "adam_beta2=0.999,\n",
      "adam_epsilon=1e-08,\n",
      "auto_find_batch_size=False,\n",
      "bf16=False,\n",
      "bf16_full_eval=False,\n",
      "data_seed=None,\n",
      "dataloader_drop_last=False,\n",
      "dataloader_num_workers=0,\n",
      "dataloader_persistent_workers=False,\n",
      "dataloader_pin_memory=True,\n",
      "dataloader_prefetch_factor=None,\n",
      "ddp_backend=None,\n",
      "ddp_broadcast_buffers=None,\n",
      "ddp_bucket_cap_mb=None,\n",
      "ddp_find_unused_parameters=None,\n",
      "ddp_timeout=1800,\n",
      "debug=[],\n",
      "deepspeed=config/ds_config_zero3.json,\n",
      "disable_tqdm=False,\n",
      "dispatch_batches=None,\n",
      "do_eval=False,\n",
      "do_predict=False,\n",
      "do_train=True,\n",
      "eval_accumulation_steps=None,\n",
      "eval_delay=0,\n",
      "eval_steps=None,\n",
      "evaluation_strategy=IntervalStrategy.NO,\n",
      "fp16=True,\n",
      "fp16_backend=auto,\n",
      "fp16_full_eval=False,\n",
      "fp16_opt_level=O1,\n",
      "fsdp=[],\n",
      "fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False},\n",
      "fsdp_min_num_params=0,\n",
      "fsdp_transformer_layer_cls_to_wrap=None,\n",
      "full_determinism=False,\n",
      "generation_config=None,\n",
      "generation_max_length=None,\n",
      "generation_num_beams=None,\n",
      "gradient_accumulation_steps=1,\n",
      "gradient_checkpointing=False,\n",
      "gradient_checkpointing_kwargs=None,\n",
      "greater_is_better=None,\n",
      "group_by_length=False,\n",
      "half_precision_backend=auto,\n",
      "hub_always_push=False,\n",
      "hub_model_id=None,\n",
      "hub_private_repo=False,\n",
      "hub_strategy=HubStrategy.EVERY_SAVE,\n",
      "hub_token=<HUB_TOKEN>,\n",
      "ignore_data_skip=False,\n",
      "include_inputs_for_metrics=False,\n",
      "include_num_input_tokens_seen=False,\n",
      "include_tokens_per_second=False,\n",
      "jit_mode_eval=False,\n",
      "label_names=None,\n",
      "label_smoothing_factor=0.0,\n",
      "learning_rate=5e-05,\n",
      "length_column_name=length,\n",
      "load_best_model_at_end=False,\n",
      "local_rank=0,\n",
      "log_level=passive,\n",
      "log_level_replica=warning,\n",
      "log_on_each_node=True,\n",
      "logging_dir=output_dir/runs/Feb17_23-17-43_ai-model,\n",
      "logging_first_step=False,\n",
      "logging_nan_inf_filter=True,\n",
      "logging_steps=500,\n",
      "logging_strategy=IntervalStrategy.STEPS,\n",
      "lr_scheduler_kwargs={},\n",
      "lr_scheduler_type=SchedulerType.LINEAR,\n",
      "max_grad_norm=1.0,\n",
      "max_steps=-1,\n",
      "metric_for_best_model=None,\n",
      "mp_parameters=,\n",
      "neftune_noise_alpha=None,\n",
      "no_cuda=False,\n",
      "num_train_epochs=1.0,\n",
      "optim=OptimizerNames.ADAMW_TORCH,\n",
      "optim_args=None,\n",
      "output_dir=output_dir,\n",
      "overwrite_output_dir=True,\n",
      "past_index=-1,\n",
      "per_device_eval_batch_size=8,\n",
      "per_device_train_batch_size=1,\n",
      "predict_with_generate=False,\n",
      "prediction_loss_only=False,\n",
      "push_to_hub=False,\n",
      "push_to_hub_model_id=None,\n",
      "push_to_hub_organization=None,\n",
      "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
      "ray_scope=last,\n",
      "remove_unused_columns=True,\n",
      "report_to=[],\n",
      "resume_from_checkpoint=None,\n",
      "run_name=output_dir,\n",
      "save_on_each_node=False,\n",
      "save_only_model=False,\n",
      "save_safetensors=True,\n",
      "save_steps=500,\n",
      "save_strategy=IntervalStrategy.STEPS,\n",
      "save_total_limit=None,\n",
      "seed=42,\n",
      "skip_memory_metrics=True,\n",
      "sortish_sampler=False,\n",
      "split_batches=None,\n",
      "tf32=None,\n",
      "torch_compile=False,\n",
      "torch_compile_backend=None,\n",
      "torch_compile_mode=None,\n",
      "torchdynamo=None,\n",
      "tpu_metrics_debug=False,\n",
      "tpu_num_cores=None,\n",
      "use_cpu=False,\n",
      "use_ipex=False,\n",
      "use_legacy_prediction_loop=False,\n",
      "use_mps_device=False,\n",
      "warmup_ratio=0.0,\n",
      "warmup_steps=0,\n",
      "weight_decay=0.0,\n",
      ")\n",
      "02/17/2024 23:17:44 - WARNING - __main__ - You're running a t5 model but didn't provide a source prefix, which is expected, e.g. with `--source_prefix 'translate English to German: ' `\n",
      "Overwrite dataset info from restored data version if exists.\n",
      "02/17/2024 23:17:49 - INFO - datasets.builder - Overwrite dataset info from restored data version if exists.\n",
      "Loading Dataset info from /home/xuwei/.cache/huggingface/datasets/wmt16/ro-en/1.0.0/27ea1f6483dca29955adc6a9e7d8a3556fbb1aea\n",
      "02/17/2024 23:17:49 - INFO - datasets.info - Loading Dataset info from /home/xuwei/.cache/huggingface/datasets/wmt16/ro-en/1.0.0/27ea1f6483dca29955adc6a9e7d8a3556fbb1aea\n",
      "Found cached dataset wmt16 (/home/xuwei/.cache/huggingface/datasets/wmt16/ro-en/1.0.0/27ea1f6483dca29955adc6a9e7d8a3556fbb1aea)\n",
      "02/17/2024 23:17:49 - INFO - datasets.builder - Found cached dataset wmt16 (/home/xuwei/.cache/huggingface/datasets/wmt16/ro-en/1.0.0/27ea1f6483dca29955adc6a9e7d8a3556fbb1aea)\n",
      "Loading Dataset info from /home/xuwei/.cache/huggingface/datasets/wmt16/ro-en/1.0.0/27ea1f6483dca29955adc6a9e7d8a3556fbb1aea\n",
      "02/17/2024 23:17:49 - INFO - datasets.info - Loading Dataset info from /home/xuwei/.cache/huggingface/datasets/wmt16/ro-en/1.0.0/27ea1f6483dca29955adc6a9e7d8a3556fbb1aea\n",
      "[INFO|configuration_utils.py:728] 2024-02-17 23:17:50,985 >> loading configuration file config.json from cache at /home/xuwei/.cache/huggingface/hub/models--t5-3b/snapshots/bed96aab9ee46012a5046386105ee5fd0ac572f0/config.json\n",
      "[INFO|configuration_utils.py:791] 2024-02-17 23:17:50,994 >> Model config T5Config {\n",
      "  \"_name_or_path\": \"t5-3b\",\n",
      "  \"architectures\": [\n",
      "    \"T5WithLMHeadModel\"\n",
      "  ],\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_ff\": 16384,\n",
      "  \"d_kv\": 128,\n",
      "  \"d_model\": 1024,\n",
      "  \"decoder_start_token_id\": 0,\n",
      "  \"dense_act_fn\": \"relu\",\n",
      "  \"dropout_rate\": 0.1,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"feed_forward_proj\": \"relu\",\n",
      "  \"initializer_factor\": 1.0,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"is_gated_act\": false,\n",
      "  \"layer_norm_epsilon\": 1e-06,\n",
      "  \"model_type\": \"t5\",\n",
      "  \"n_positions\": 512,\n",
      "  \"num_decoder_layers\": 24,\n",
      "  \"num_heads\": 32,\n",
      "  \"num_layers\": 24,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"relative_attention_max_distance\": 128,\n",
      "  \"relative_attention_num_buckets\": 32,\n",
      "  \"task_specific_params\": {\n",
      "    \"summarization\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"length_penalty\": 2.0,\n",
      "      \"max_length\": 200,\n",
      "      \"min_length\": 30,\n",
      "      \"no_repeat_ngram_size\": 3,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"summarize: \"\n",
      "    },\n",
      "    \"translation_en_to_de\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to German: \"\n",
      "    },\n",
      "    \"translation_en_to_fr\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to French: \"\n",
      "    },\n",
      "    \"translation_en_to_ro\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to Romanian: \"\n",
      "    }\n",
      "  },\n",
      "  \"transformers_version\": \"4.38.0.dev0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32128\n",
      "}\n",
      "\n",
      "[INFO|tokenization_auto.py:610] 2024-02-17 23:17:51,290 >> Could not locate the tokenizer configuration file, will try to use the model config instead.\n",
      "[INFO|configuration_utils.py:728] 2024-02-17 23:17:51,577 >> loading configuration file config.json from cache at /home/xuwei/.cache/huggingface/hub/models--t5-3b/snapshots/bed96aab9ee46012a5046386105ee5fd0ac572f0/config.json\n",
      "[INFO|configuration_utils.py:791] 2024-02-17 23:17:51,580 >> Model config T5Config {\n",
      "  \"_name_or_path\": \"t5-3b\",\n",
      "  \"architectures\": [\n",
      "    \"T5WithLMHeadModel\"\n",
      "  ],\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_ff\": 16384,\n",
      "  \"d_kv\": 128,\n",
      "  \"d_model\": 1024,\n",
      "  \"decoder_start_token_id\": 0,\n",
      "  \"dense_act_fn\": \"relu\",\n",
      "  \"dropout_rate\": 0.1,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"feed_forward_proj\": \"relu\",\n",
      "  \"initializer_factor\": 1.0,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"is_gated_act\": false,\n",
      "  \"layer_norm_epsilon\": 1e-06,\n",
      "  \"model_type\": \"t5\",\n",
      "  \"n_positions\": 512,\n",
      "  \"num_decoder_layers\": 24,\n",
      "  \"num_heads\": 32,\n",
      "  \"num_layers\": 24,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"relative_attention_max_distance\": 128,\n",
      "  \"relative_attention_num_buckets\": 32,\n",
      "  \"task_specific_params\": {\n",
      "    \"summarization\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"length_penalty\": 2.0,\n",
      "      \"max_length\": 200,\n",
      "      \"min_length\": 30,\n",
      "      \"no_repeat_ngram_size\": 3,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"summarize: \"\n",
      "    },\n",
      "    \"translation_en_to_de\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to German: \"\n",
      "    },\n",
      "    \"translation_en_to_fr\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to French: \"\n",
      "    },\n",
      "    \"translation_en_to_ro\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to Romanian: \"\n",
      "    }\n",
      "  },\n",
      "  \"transformers_version\": \"4.38.0.dev0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32128\n",
      "}\n",
      "\n",
      "[INFO|tokenization_utils_base.py:2046] 2024-02-17 23:17:52,170 >> loading file spiece.model from cache at /home/xuwei/.cache/huggingface/hub/models--t5-3b/snapshots/bed96aab9ee46012a5046386105ee5fd0ac572f0/spiece.model\n",
      "[INFO|tokenization_utils_base.py:2046] 2024-02-17 23:17:52,170 >> loading file tokenizer.json from cache at /home/xuwei/.cache/huggingface/hub/models--t5-3b/snapshots/bed96aab9ee46012a5046386105ee5fd0ac572f0/tokenizer.json\n",
      "[INFO|tokenization_utils_base.py:2046] 2024-02-17 23:17:52,170 >> loading file added_tokens.json from cache at None\n",
      "[INFO|tokenization_utils_base.py:2046] 2024-02-17 23:17:52,170 >> loading file special_tokens_map.json from cache at None\n",
      "[INFO|tokenization_utils_base.py:2046] 2024-02-17 23:17:52,170 >> loading file tokenizer_config.json from cache at None\n",
      "[INFO|configuration_utils.py:728] 2024-02-17 23:17:52,170 >> loading configuration file config.json from cache at /home/xuwei/.cache/huggingface/hub/models--t5-3b/snapshots/bed96aab9ee46012a5046386105ee5fd0ac572f0/config.json\n",
      "[INFO|configuration_utils.py:791] 2024-02-17 23:17:52,171 >> Model config T5Config {\n",
      "  \"_name_or_path\": \"t5-3b\",\n",
      "  \"architectures\": [\n",
      "    \"T5WithLMHeadModel\"\n",
      "  ],\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_ff\": 16384,\n",
      "  \"d_kv\": 128,\n",
      "  \"d_model\": 1024,\n",
      "  \"decoder_start_token_id\": 0,\n",
      "  \"dense_act_fn\": \"relu\",\n",
      "  \"dropout_rate\": 0.1,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"feed_forward_proj\": \"relu\",\n",
      "  \"initializer_factor\": 1.0,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"is_gated_act\": false,\n",
      "  \"layer_norm_epsilon\": 1e-06,\n",
      "  \"model_type\": \"t5\",\n",
      "  \"n_positions\": 512,\n",
      "  \"num_decoder_layers\": 24,\n",
      "  \"num_heads\": 32,\n",
      "  \"num_layers\": 24,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"relative_attention_max_distance\": 128,\n",
      "  \"relative_attention_num_buckets\": 32,\n",
      "  \"task_specific_params\": {\n",
      "    \"summarization\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"length_penalty\": 2.0,\n",
      "      \"max_length\": 200,\n",
      "      \"min_length\": 30,\n",
      "      \"no_repeat_ngram_size\": 3,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"summarize: \"\n",
      "    },\n",
      "    \"translation_en_to_de\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to German: \"\n",
      "    },\n",
      "    \"translation_en_to_fr\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to French: \"\n",
      "    },\n",
      "    \"translation_en_to_ro\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to Romanian: \"\n",
      "    }\n",
      "  },\n",
      "  \"transformers_version\": \"4.38.0.dev0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32128\n",
      "}\n",
      "\n",
      "[INFO|modeling_utils.py:3257] 2024-02-17 23:17:52,336 >> loading weights file model.safetensors from cache at /home/xuwei/.cache/huggingface/hub/models--t5-3b/snapshots/bed96aab9ee46012a5046386105ee5fd0ac572f0/model.safetensors\n",
      "[INFO|modeling_utils.py:3363] 2024-02-17 23:17:52,791 >> Detected DeepSpeed ZeRO-3: activating zero.init() for this model\n",
      "[INFO|configuration_utils.py:840] 2024-02-17 23:17:52,797 >> Generate config GenerationConfig {\n",
      "  \"decoder_start_token_id\": 0,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"pad_token_id\": 0\n",
      "}\n",
      "\n",
      "[2024-02-17 23:17:57,798] [INFO] [partition_parameters.py:348:__exit__] finished initializing model - num_params = 510, num_elems = 2.88B\n",
      "[INFO|modeling_utils.py:3992] 2024-02-17 23:18:40,585 >> All model checkpoint weights were used when initializing T5ForConditionalGeneration.\n",
      "\n",
      "[INFO|modeling_utils.py:4000] 2024-02-17 23:18:40,585 >> All the weights of T5ForConditionalGeneration were initialized from the model checkpoint at t5-3b.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use T5ForConditionalGeneration for predictions without further training.\n",
      "[INFO|modeling_utils.py:3544] 2024-02-17 23:18:40,937 >> Generation config file not found, using a generation config created from the model config.\n",
      "[INFO|modeling_utils.py:1875] 2024-02-17 23:18:41,276 >> You are resizing the embedding layer without providing a `pad_to_multiple_of` parameter. This means that the new embedding dimension will be 32100. This might induce some performance reduction as *Tensor Cores* will not be available. For more details about this, or help on choosing the correct value for resizing, refer to this guide: https://docs.nvidia.com/deeplearning/performance/dl-performance-matrix-multiplication/index.html#requirements-tc\n",
      "Loading cached processed dataset at /home/xuwei/.cache/huggingface/datasets/wmt16/ro-en/1.0.0/27ea1f6483dca29955adc6a9e7d8a3556fbb1aea/cache-b34f85c1f43d4258.arrow\n",
      "02/17/2024 23:18:41 - INFO - datasets.arrow_dataset - Loading cached processed dataset at /home/xuwei/.cache/huggingface/datasets/wmt16/ro-en/1.0.0/27ea1f6483dca29955adc6a9e7d8a3556fbb1aea/cache-b34f85c1f43d4258.arrow\n",
      "02/17/2024 23:18:43 - WARNING - accelerate.utils.other - Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n",
      "[INFO|trainer.py:594] 2024-02-17 23:18:43,605 >> Using auto half precision backend\n",
      "[2024-02-17 23:18:43,815] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed info: version=0.12.6, git-hash=unknown, git-branch=unknown\n",
      "[2024-02-17 23:18:43,842] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False\n",
      "Using /home/xuwei/.cache/torch_extensions/py311_cu118 as PyTorch extensions root...\n",
      "Detected CUDA files, patching ldflags\n",
      "Emitting ninja build file /home/xuwei/.cache/torch_extensions/py311_cu118/cpu_adam/build.ninja...\n",
      "Building extension module cpu_adam...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "ninja: no work to do.\n",
      "Loading extension module cpu_adam...\n",
      "Time to load cpu_adam op: 2.4709126949310303 seconds\n",
      "Adam Optimizer #0 is created with AVX512 arithmetic capability.\n",
      "Config: alpha=0.000050, betas=(0.900000, 0.999000), weight_decay=0.000000, adam_w=1\n",
      "[2024-02-17 23:18:50,416] [INFO] [logging.py:96:log_dist] [Rank 0] Using DeepSpeed Optimizer param name adamw as basic optimizer\n",
      "[2024-02-17 23:18:50,416] [INFO] [logging.py:96:log_dist] [Rank 0] Removing param_group that has no 'params' in the basic Optimizer\n",
      "[2024-02-17 23:18:50,475] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Basic Optimizer = DeepSpeedCPUAdam\n",
      "[2024-02-17 23:18:50,475] [INFO] [utils.py:56:is_zero_supported_optimizer] Checking ZeRO support for optimizer=DeepSpeedCPUAdam type=<class 'deepspeed.ops.adam.cpu_adam.DeepSpeedCPUAdam'>\n",
      "[2024-02-17 23:18:50,475] [INFO] [logging.py:96:log_dist] [Rank 0] Creating fp16 ZeRO stage 3 optimizer, MiCS is enabled False, Hierarchical params gather False\n",
      "[2024-02-17 23:18:50,475] [INFO] [logging.py:96:log_dist] [Rank 0] Creating torch.float16 ZeRO stage 3 optimizer\n",
      "[2024-02-17 23:18:50,675] [INFO] [utils.py:791:see_memory_usage] Stage 3 initialize beginning\n",
      "[2024-02-17 23:18:50,676] [INFO] [utils.py:792:see_memory_usage] MA 5.31 GB         Max_MA 5.5 GB         CA 5.44 GB         Max_CA 6 GB \n",
      "[2024-02-17 23:18:50,677] [INFO] [utils.py:799:see_memory_usage] CPU Virtual Memory:  used = 23.93 GB, percent = 38.1%\n",
      "[2024-02-17 23:18:50,683] [INFO] [stage3.py:127:__init__] Reduce bucket size 134217720\n",
      "[2024-02-17 23:18:50,683] [INFO] [stage3.py:128:__init__] Prefetch bucket size 52428800\n",
      "[2024-02-17 23:18:50,888] [INFO] [utils.py:791:see_memory_usage] DeepSpeedZeRoOffload initialize [begin]\n",
      "[2024-02-17 23:18:50,889] [INFO] [utils.py:792:see_memory_usage] MA 5.31 GB         Max_MA 5.31 GB         CA 5.44 GB         Max_CA 5 GB \n",
      "[2024-02-17 23:18:50,889] [INFO] [utils.py:799:see_memory_usage] CPU Virtual Memory:  used = 23.93 GB, percent = 38.1%\n",
      "Parameter Offload: Total persistent parameters: 126976 in 124 params\n",
      "[2024-02-17 23:18:51,170] [INFO] [utils.py:791:see_memory_usage] DeepSpeedZeRoOffload initialize [end]\n",
      "[2024-02-17 23:18:51,171] [INFO] [utils.py:792:see_memory_usage] MA 5.31 GB         Max_MA 5.37 GB         CA 5.44 GB         Max_CA 5 GB \n",
      "[2024-02-17 23:18:51,172] [INFO] [utils.py:799:see_memory_usage] CPU Virtual Memory:  used = 23.93 GB, percent = 38.1%\n",
      "[2024-02-17 23:18:51,386] [INFO] [utils.py:791:see_memory_usage] Before creating fp16 partitions\n",
      "[2024-02-17 23:18:51,387] [INFO] [utils.py:792:see_memory_usage] MA 5.31 GB         Max_MA 5.31 GB         CA 5.44 GB         Max_CA 5 GB \n",
      "[2024-02-17 23:18:51,388] [INFO] [utils.py:799:see_memory_usage] CPU Virtual Memory:  used = 23.94 GB, percent = 38.1%\n",
      "[2024-02-17 23:18:57,518] [INFO] [utils.py:791:see_memory_usage] After creating fp16 partitions: 29\n",
      "[2024-02-17 23:18:57,519] [INFO] [utils.py:792:see_memory_usage] MA 5.31 GB         Max_MA 5.31 GB         CA 5.31 GB         Max_CA 5 GB \n",
      "[2024-02-17 23:18:57,519] [INFO] [utils.py:799:see_memory_usage] CPU Virtual Memory:  used = 23.94 GB, percent = 38.1%\n",
      "[2024-02-17 23:18:57,683] [INFO] [utils.py:791:see_memory_usage] Before creating fp32 partitions\n",
      "[2024-02-17 23:18:57,684] [INFO] [utils.py:792:see_memory_usage] MA 5.31 GB         Max_MA 5.31 GB         CA 5.31 GB         Max_CA 5 GB \n",
      "[2024-02-17 23:18:57,684] [INFO] [utils.py:799:see_memory_usage] CPU Virtual Memory:  used = 23.94 GB, percent = 38.1%\n",
      "[2024-02-17 23:19:06,688] [INFO] [utils.py:791:see_memory_usage] After creating fp32 partitions\n",
      "[2024-02-17 23:19:06,689] [INFO] [utils.py:792:see_memory_usage] MA 5.31 GB         Max_MA 5.31 GB         CA 5.31 GB         Max_CA 5 GB \n",
      "[2024-02-17 23:19:06,689] [INFO] [utils.py:799:see_memory_usage] CPU Virtual Memory:  used = 34.58 GB, percent = 55.1%\n",
      "[2024-02-17 23:19:06,882] [INFO] [utils.py:791:see_memory_usage] Before initializing optimizer states\n",
      "[2024-02-17 23:19:06,883] [INFO] [utils.py:792:see_memory_usage] MA 5.31 GB         Max_MA 5.31 GB         CA 5.31 GB         Max_CA 5 GB \n",
      "[2024-02-17 23:19:06,884] [INFO] [utils.py:799:see_memory_usage] CPU Virtual Memory:  used = 34.58 GB, percent = 55.1%\n",
      "[2024-02-17 23:19:32,911] [INFO] [utils.py:791:see_memory_usage] After initializing optimizer states\n",
      "[2024-02-17 23:19:32,929] [INFO] [utils.py:792:see_memory_usage] MA 5.31 GB         Max_MA 5.31 GB         CA 5.31 GB         Max_CA 5 GB \n",
      "[2024-02-17 23:19:32,930] [INFO] [utils.py:799:see_memory_usage] CPU Virtual Memory:  used = 62.23 GB, percent = 99.1%\n",
      "[2024-02-17 23:19:33,002] [INFO] [stage3.py:479:_setup_for_real_optimizer] optimizer state initialized\n",
      "[2024-02-17 23:19:55,825] [INFO] [utils.py:791:see_memory_usage] After initializing ZeRO optimizer\n",
      "[2024-02-17 23:19:55,831] [INFO] [utils.py:792:see_memory_usage] MA 5.56 GB         Max_MA 5.68 GB         CA 5.69 GB         Max_CA 6 GB \n",
      "[2024-02-17 23:19:55,832] [INFO] [utils.py:799:see_memory_usage] CPU Virtual Memory:  used = 62.38 GB, percent = 99.3%\n",
      "[2024-02-17 23:19:55,832] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Final Optimizer = adamw\n",
      "[2024-02-17 23:19:55,834] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed using configured LR scheduler = WarmupLR\n",
      "[2024-02-17 23:19:55,834] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed LR Scheduler = <deepspeed.runtime.lr_schedules.WarmupLR object at 0x7f859720f050>\n",
      "[2024-02-17 23:19:55,834] [INFO] [logging.py:96:log_dist] [Rank 0] step=0, skipped=0, lr=[5e-05], mom=[[0.9, 0.999]]\n",
      "[2024-02-17 23:19:55,837] [INFO] [config.py:984:print] DeepSpeedEngine configuration:\n",
      "[2024-02-17 23:19:55,838] [INFO] [config.py:988:print]   activation_checkpointing_config  {\n",
      "    \"partition_activations\": false, \n",
      "    \"contiguous_memory_optimization\": false, \n",
      "    \"cpu_checkpointing\": false, \n",
      "    \"number_checkpoints\": null, \n",
      "    \"synchronize_checkpoint_boundary\": false, \n",
      "    \"profile\": false\n",
      "}\n",
      "[2024-02-17 23:19:55,838] [INFO] [config.py:988:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}\n",
      "[2024-02-17 23:19:55,838] [INFO] [config.py:988:print]   amp_enabled .................. False\n",
      "[2024-02-17 23:19:55,838] [INFO] [config.py:988:print]   amp_params ................... False\n",
      "[2024-02-17 23:19:55,839] [INFO] [config.py:988:print]   autotuning_config ............ {\n",
      "    \"enabled\": false, \n",
      "    \"start_step\": null, \n",
      "    \"end_step\": null, \n",
      "    \"metric_path\": null, \n",
      "    \"arg_mappings\": null, \n",
      "    \"metric\": \"throughput\", \n",
      "    \"model_info\": null, \n",
      "    \"results_dir\": \"autotuning_results\", \n",
      "    \"exps_dir\": \"autotuning_exps\", \n",
      "    \"overwrite\": true, \n",
      "    \"fast\": true, \n",
      "    \"start_profile_step\": 3, \n",
      "    \"end_profile_step\": 5, \n",
      "    \"tuner_type\": \"gridsearch\", \n",
      "    \"tuner_early_stopping\": 5, \n",
      "    \"tuner_num_trials\": 50, \n",
      "    \"model_info_path\": null, \n",
      "    \"mp_size\": 1, \n",
      "    \"max_train_batch_size\": null, \n",
      "    \"min_train_batch_size\": 1, \n",
      "    \"max_train_micro_batch_size_per_gpu\": 1.024000e+03, \n",
      "    \"min_train_micro_batch_size_per_gpu\": 1, \n",
      "    \"num_tuning_micro_batch_sizes\": 3\n",
      "}\n",
      "[2024-02-17 23:19:55,840] [INFO] [config.py:988:print]   bfloat16_enabled ............. False\n",
      "[2024-02-17 23:19:55,840] [INFO] [config.py:988:print]   checkpoint_parallel_write_pipeline  False\n",
      "[2024-02-17 23:19:55,840] [INFO] [config.py:988:print]   checkpoint_tag_validation_enabled  True\n",
      "[2024-02-17 23:19:55,840] [INFO] [config.py:988:print]   checkpoint_tag_validation_fail  False\n",
      "[2024-02-17 23:19:55,840] [INFO] [config.py:988:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7f85f023b510>\n",
      "[2024-02-17 23:19:55,840] [INFO] [config.py:988:print]   communication_data_type ...... None\n",
      "[2024-02-17 23:19:55,840] [INFO] [config.py:988:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}\n",
      "[2024-02-17 23:19:55,840] [INFO] [config.py:988:print]   curriculum_enabled_legacy .... False\n",
      "[2024-02-17 23:19:55,840] [INFO] [config.py:988:print]   curriculum_params_legacy ..... False\n",
      "[2024-02-17 23:19:55,840] [INFO] [config.py:988:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}\n",
      "[2024-02-17 23:19:55,840] [INFO] [config.py:988:print]   data_efficiency_enabled ...... False\n",
      "[2024-02-17 23:19:55,840] [INFO] [config.py:988:print]   dataloader_drop_last ......... False\n",
      "[2024-02-17 23:19:55,840] [INFO] [config.py:988:print]   disable_allgather ............ False\n",
      "[2024-02-17 23:19:55,840] [INFO] [config.py:988:print]   dump_state ................... False\n",
      "[2024-02-17 23:19:55,840] [INFO] [config.py:988:print]   dynamic_loss_scale_args ...... {'init_scale': 65536, 'scale_window': 1000, 'delayed_shift': 2, 'consecutive_hysteresis': False, 'min_scale': 1}\n",
      "[2024-02-17 23:19:55,840] [INFO] [config.py:988:print]   eigenvalue_enabled ........... False\n",
      "[2024-02-17 23:19:55,840] [INFO] [config.py:988:print]   eigenvalue_gas_boundary_resolution  1\n",
      "[2024-02-17 23:19:55,840] [INFO] [config.py:988:print]   eigenvalue_layer_name ........ bert.encoder.layer\n",
      "[2024-02-17 23:19:55,840] [INFO] [config.py:988:print]   eigenvalue_layer_num ......... 0\n",
      "[2024-02-17 23:19:55,840] [INFO] [config.py:988:print]   eigenvalue_max_iter .......... 100\n",
      "[2024-02-17 23:19:55,840] [INFO] [config.py:988:print]   eigenvalue_stability ......... 1e-06\n",
      "[2024-02-17 23:19:55,841] [INFO] [config.py:988:print]   eigenvalue_tol ............... 0.01\n",
      "[2024-02-17 23:19:55,841] [INFO] [config.py:988:print]   eigenvalue_verbose ........... False\n",
      "[2024-02-17 23:19:55,841] [INFO] [config.py:988:print]   elasticity_enabled ........... False\n",
      "[2024-02-17 23:19:55,841] [INFO] [config.py:988:print]   flops_profiler_config ........ {\n",
      "    \"enabled\": false, \n",
      "    \"recompute_fwd_factor\": 0.0, \n",
      "    \"profile_step\": 1, \n",
      "    \"module_depth\": -1, \n",
      "    \"top_modules\": 1, \n",
      "    \"detailed\": true, \n",
      "    \"output_file\": null\n",
      "}\n",
      "[2024-02-17 23:19:55,841] [INFO] [config.py:988:print]   fp16_auto_cast ............... False\n",
      "[2024-02-17 23:19:55,841] [INFO] [config.py:988:print]   fp16_enabled ................. True\n",
      "[2024-02-17 23:19:55,841] [INFO] [config.py:988:print]   fp16_master_weights_and_gradients  False\n",
      "[2024-02-17 23:19:55,841] [INFO] [config.py:988:print]   global_rank .................. 0\n",
      "[2024-02-17 23:19:55,841] [INFO] [config.py:988:print]   grad_accum_dtype ............. None\n",
      "[2024-02-17 23:19:55,841] [INFO] [config.py:988:print]   gradient_accumulation_steps .. 1\n",
      "[2024-02-17 23:19:55,841] [INFO] [config.py:988:print]   gradient_clipping ............ 1.0\n",
      "[2024-02-17 23:19:55,841] [INFO] [config.py:988:print]   gradient_predivide_factor .... 1.0\n",
      "[2024-02-17 23:19:55,841] [INFO] [config.py:988:print]   graph_harvesting ............. False\n",
      "[2024-02-17 23:19:55,846] [INFO] [config.py:988:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8\n",
      "[2024-02-17 23:19:55,846] [INFO] [config.py:988:print]   initial_dynamic_scale ........ 65536\n",
      "[2024-02-17 23:19:55,846] [INFO] [config.py:988:print]   load_universal_checkpoint .... False\n",
      "[2024-02-17 23:19:55,846] [INFO] [config.py:988:print]   loss_scale ................... 0\n",
      "[2024-02-17 23:19:55,846] [INFO] [config.py:988:print]   memory_breakdown ............. False\n",
      "[2024-02-17 23:19:55,846] [INFO] [config.py:988:print]   mics_hierarchial_params_gather  False\n",
      "[2024-02-17 23:19:55,846] [INFO] [config.py:988:print]   mics_shard_size .............. -1\n",
      "[2024-02-17 23:19:55,846] [INFO] [config.py:988:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') enabled=False\n",
      "[2024-02-17 23:19:55,847] [INFO] [config.py:988:print]   nebula_config ................ {\n",
      "    \"enabled\": false, \n",
      "    \"persistent_storage_path\": null, \n",
      "    \"persistent_time_interval\": 100, \n",
      "    \"num_of_version_in_retention\": 2, \n",
      "    \"enable_nebula_load\": true, \n",
      "    \"load_path\": null\n",
      "}\n",
      "[2024-02-17 23:19:55,847] [INFO] [config.py:988:print]   optimizer_legacy_fusion ...... False\n",
      "[2024-02-17 23:19:55,847] [INFO] [config.py:988:print]   optimizer_name ............... adamw\n",
      "[2024-02-17 23:19:55,847] [INFO] [config.py:988:print]   optimizer_params ............. {'lr': 5e-05, 'betas': [0.9, 0.999], 'eps': 1e-08, 'weight_decay': 0.0}\n",
      "[2024-02-17 23:19:55,847] [INFO] [config.py:988:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0, 'pipe_partitioned': True, 'grad_partitioned': True}\n",
      "[2024-02-17 23:19:55,847] [INFO] [config.py:988:print]   pld_enabled .................. False\n",
      "[2024-02-17 23:19:55,847] [INFO] [config.py:988:print]   pld_params ................... False\n",
      "[2024-02-17 23:19:55,847] [INFO] [config.py:988:print]   prescale_gradients ........... False\n",
      "[2024-02-17 23:19:55,847] [INFO] [config.py:988:print]   scheduler_name ............... WarmupLR\n",
      "[2024-02-17 23:19:55,847] [INFO] [config.py:988:print]   scheduler_params ............. {'warmup_min_lr': 0, 'warmup_max_lr': 5e-05, 'warmup_num_steps': 0}\n",
      "[2024-02-17 23:19:55,847] [INFO] [config.py:988:print]   seq_parallel_communication_data_type  torch.float32\n",
      "[2024-02-17 23:19:55,847] [INFO] [config.py:988:print]   sparse_attention ............. None\n",
      "[2024-02-17 23:19:55,847] [INFO] [config.py:988:print]   sparse_gradients_enabled ..... False\n",
      "[2024-02-17 23:19:55,847] [INFO] [config.py:988:print]   steps_per_print .............. inf\n",
      "[2024-02-17 23:19:55,847] [INFO] [config.py:988:print]   train_batch_size ............. 1\n",
      "[2024-02-17 23:19:55,847] [INFO] [config.py:988:print]   train_micro_batch_size_per_gpu  1\n",
      "[2024-02-17 23:19:55,847] [INFO] [config.py:988:print]   use_data_before_expert_parallel_  False\n",
      "[2024-02-17 23:19:55,847] [INFO] [config.py:988:print]   use_node_local_storage ....... False\n",
      "[2024-02-17 23:19:55,847] [INFO] [config.py:988:print]   wall_clock_breakdown ......... False\n",
      "[2024-02-17 23:19:55,847] [INFO] [config.py:988:print]   weight_quantization_config ... None\n",
      "[2024-02-17 23:19:55,847] [INFO] [config.py:988:print]   world_size ................... 1\n",
      "[2024-02-17 23:19:55,847] [INFO] [config.py:988:print]   zero_allow_untested_optimizer  False\n",
      "[2024-02-17 23:19:55,848] [INFO] [config.py:988:print]   zero_config .................. stage=3 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=134217720 use_multi_rank_bucket_allreduce=True allgather_partitions=True allgather_bucket_size=500,000,000 overlap_comm=True load_from_fp32_weights=True elastic_checkpoint=False offload_param=None offload_optimizer=DeepSpeedZeroOffloadOptimizerConfig(device='cpu', nvme_path=None, buffer_count=4, pin_memory=False, pipeline=False, pipeline_read=False, pipeline_write=False, fast_init=False, ratio=1.0) sub_group_size=100000000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=52428800 param_persistence_threshold=100,000 model_persistence_threshold=sys.maxsize max_live_parameters=1000000000 max_reuse_distance=1000000000 gather_16bit_weights_on_model_save=True stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_nontrainable_weights=False zero_quantized_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True pipeline_loading_checkpoint=False override_module_apply=True\n",
      "[2024-02-17 23:19:55,848] [INFO] [config.py:988:print]   zero_enabled ................. True\n",
      "[2024-02-17 23:19:55,848] [INFO] [config.py:988:print]   zero_force_ds_cpu_optimizer .. True\n",
      "[2024-02-17 23:19:55,848] [INFO] [config.py:988:print]   zero_optimization_stage ...... 3\n",
      "[2024-02-17 23:19:55,848] [INFO] [config.py:974:print_user_config]   json = {\n",
      "    \"fp16\": {\n",
      "        \"enabled\": true, \n",
      "        \"loss_scale\": 0, \n",
      "        \"loss_scale_window\": 1000, \n",
      "        \"initial_scale_power\": 16, \n",
      "        \"hysteresis\": 2, \n",
      "        \"min_loss_scale\": 1\n",
      "    }, \n",
      "    \"bf16\": {\n",
      "        \"enabled\": false\n",
      "    }, \n",
      "    \"optimizer\": {\n",
      "        \"type\": \"AdamW\", \n",
      "        \"params\": {\n",
      "            \"lr\": 5e-05, \n",
      "            \"betas\": [0.9, 0.999], \n",
      "            \"eps\": 1e-08, \n",
      "            \"weight_decay\": 0.0\n",
      "        }\n",
      "    }, \n",
      "    \"scheduler\": {\n",
      "        \"type\": \"WarmupLR\", \n",
      "        \"params\": {\n",
      "            \"warmup_min_lr\": 0, \n",
      "            \"warmup_max_lr\": 5e-05, \n",
      "            \"warmup_num_steps\": 0\n",
      "        }\n",
      "    }, \n",
      "    \"zero_optimization\": {\n",
      "        \"stage\": 3, \n",
      "        \"offload_optimizer\": {\n",
      "            \"device\": \"cpu\", \n",
      "            \"pin_memory\": false\n",
      "        }, \n",
      "        \"overlap_comm\": true, \n",
      "        \"contiguous_gradients\": true, \n",
      "        \"sub_group_size\": 1.000000e+08, \n",
      "        \"reduce_bucket_size\": 1.342177e+08, \n",
      "        \"stage3_prefetch_bucket_size\": 5.242880e+07, \n",
      "        \"stage3_max_live_parameters\": 1.000000e+09, \n",
      "        \"stage3_max_reuse_distance\": 1.000000e+09, \n",
      "        \"stage3_gather_16bit_weights_on_model_save\": true\n",
      "    }, \n",
      "    \"gradient_accumulation_steps\": 1, \n",
      "    \"gradient_clipping\": 1.0, \n",
      "    \"steps_per_print\": inf, \n",
      "    \"train_batch_size\": 1, \n",
      "    \"train_micro_batch_size_per_gpu\": 1, \n",
      "    \"wall_clock_breakdown\": false\n",
      "}\n",
      "[INFO|trainer.py:1794] 2024-02-17 23:19:55,849 >> ***** Running training *****\n",
      "[INFO|trainer.py:1795] 2024-02-17 23:19:55,849 >>   Num examples = 500\n",
      "[INFO|trainer.py:1796] 2024-02-17 23:19:55,849 >>   Num Epochs = 1\n",
      "[INFO|trainer.py:1797] 2024-02-17 23:19:55,849 >>   Instantaneous batch size per device = 1\n",
      "[INFO|trainer.py:1800] 2024-02-17 23:19:55,849 >>   Total train batch size (w. parallel, distributed & accumulation) = 1\n",
      "[INFO|trainer.py:1801] 2024-02-17 23:19:55,849 >>   Gradient Accumulation steps = 1\n",
      "[INFO|trainer.py:1802] 2024-02-17 23:19:55,849 >>   Total optimization steps = 500\n",
      "[INFO|trainer.py:1803] 2024-02-17 23:19:55,854 >>   Number of trainable parameters = 2,851,569,664\n",
      "  0%|                                                   | 0/500 [00:00<?, ?it/s][2024-02-17 23:20:46,866] [INFO] [loss_scaler.py:190:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 65536, but hysteresis is 2. Reducing hysteresis to 1\n",
      "  0%|                                         | 1/500 [00:52<7:12:52, 52.05s/it][2024-02-17 23:21:11,947] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 65536, reducing to 32768\n",
      "  0%|▏                                        | 2/500 [01:16<4:54:59, 35.54s/it][2024-02-17 23:21:29,935] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, reducing to 16384\n",
      "  1%|▏                                        | 3/500 [01:34<3:48:00, 27.53s/it][2024-02-17 23:21:46,760] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 16384, reducing to 8192\n",
      "  1%|▎                                        | 4/500 [01:50<3:12:36, 23.30s/it][2024-02-17 23:22:11,491] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 8192, reducing to 4096\n",
      "  1%|▍                                        | 5/500 [02:15<3:16:29, 23.82s/it][2024-02-17 23:22:25,817] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 4096, reducing to 2048\n",
      "  1%|▍                                        | 6/500 [02:29<2:49:31, 20.59s/it]/home/xuwei/jupyterlab/venv/lib/python3.11/site-packages/deepspeed/runtime/zero/stage3.py:1330: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)\n",
      "  total_norm_cuda = get_accelerator().FloatTensor([float(total_norm)])\n",
      "  2%|▋                                        | 8/500 [03:31<3:35:45, 26.31s/it][2024-02-17 23:23:48,366] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 2048, reducing to 1024\n",
      "  2%|▋                                        | 9/500 [03:52<3:20:35, 24.51s/it][2024-02-17 23:24:04,059] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 1024, reducing to 512\n",
      "  2%|▉                                       | 12/500 [04:52<2:58:16, 21.92s/it]"
     ]
    }
   ],
   "source": [
    "# DeepSpeed ZeRO-3 模式单 GPU 训练翻译模型（T5-Large）\n",
    "! deepspeed --num_gpus=1 translation/run_translation.py \\\n",
    "--deepspeed config/ds_config_zero3.json \\\n",
    "--model_name_or_path t5-3b --per_device_train_batch_size 1 \\\n",
    "--output_dir output_dir --overwrite_output_dir --fp16 \\\n",
    "--do_train --max_train_samples 500 --num_train_epochs 1 \\\n",
    "--dataset_name wmt16 --dataset_config \"ro-en\" \\\n",
    "--source_lang en --target_lang ro\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f77b632b-51d9-4dd9-9e79-70bd370a8b70",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
